{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8na3X2gxM7hl"
   },
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDUENjr8XIem"
   },
   "source": [
    "# Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGalBNaFNhW2",
    "outputId": "f5cefe30-4068-4e9e-dace-7db09789a7f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 231239\n",
      "Number of edges: 1777338\n"
     ]
    }
   ],
   "source": [
    "graph_path = path + 'collaboration_network.edgelist'\n",
    "G = nx.read_edgelist(graph_path, delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsVudqZuXGac"
   },
   "outputs": [],
   "source": [
    "# computes structural features for each node\n",
    "core_number = nx.core_number(G)\n",
    "avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
    "cluster = nx.clustering(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBaIkhNnW7dW"
   },
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ_wHezdPqFt"
   },
   "outputs": [],
   "source": [
    "# Store the author_id and his papers_ids in dictionary\n",
    "f = open(path + 'author_papers.txt',\"r\")\n",
    "papers_set = set()\n",
    "d = {}\n",
    "for l in f:\n",
    "    auth_paps = [paper_id.strip() for paper_id in l.split(\":\")[1].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\"\\'\",\"\").replace(\"\\\"\",\"\").split(\",\")]\n",
    "    d[l.split(\":\")[0]] = auth_paps\n",
    "f.close()\n",
    "\n",
    "np.save(path + 'author_paper.npy', d)\n",
    "\n",
    "# Store the paper_id and his tokenized abstract in a dictionary\n",
    "fw = open(path + 'abstracts_processed.txt',\"r\",encoding=\"utf8\")\n",
    "d_abstracts = {}\n",
    "for l in fw:\n",
    "    key, value = l.split('----')\n",
    "    d_abstracts[key] = value\n",
    "fw.close()\n",
    "# Store the paper_id and his raw abstract in a dictionary\n",
    "ff = open(path+\"abstracts_documents_final.txt\",\"r\",encoding=\"utf8\")\n",
    "doc_abst = {}\n",
    "for l in ff:\n",
    "    id_,text = l.split(\"----\")\n",
    "    doc_abst[id_] = text.rstrip(\"\\n\")\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C1ISZInNqms"
   },
   "source": [
    "## Co-authorship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRQYEQUWkc2F"
   },
   "source": [
    "#### Co_authorship_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH9ZIg49lKo2"
   },
   "source": [
    "This feature defines the number of common papers written between each two authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSaZ8ushNqG8"
   },
   "outputs": [],
   "source": [
    "co_authorship = {}\n",
    "for i,edge in enumerate(tqdm(G.edges)):\n",
    "    paper_author_1 = d[str(int(edge[0]))]\n",
    "    paper_author_2 = d[str(int(edge[1]))]\n",
    "    res = int(len(list(set(paper_author_1).intersection(set(paper_author_2)))))\n",
    "    if res==0:\n",
    "        co_authorship[edge] = 1\n",
    "    else: \n",
    "        co_authorship[edge] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rx11fsKPB8v"
   },
   "outputs": [],
   "source": [
    "# Create weighted graph\n",
    "G_weigthed = G.copy()\n",
    "for i,edge in enumerate(tqdm(list(G_weigthed.edges))):\n",
    "    source,target = edge\n",
    "    G_weigthed[source][target]['weight'] = co_authorship[tuple(edge)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DvgB3iHPPKk"
   },
   "outputs": [],
   "source": [
    "co_authored = {}\n",
    "for node in tqdm((list(G.nodes()))):\n",
    "    counts = []\n",
    "    edges = list(G.edges(node))\n",
    "    for edge in edges:\n",
    "        counts.append(G_weigthed[edge[0]][edge[1]]['weight'] )\n",
    "    co_authored[node] = counts\n",
    "    if len(co_authored[node])!=G.degree(node):\n",
    "        print('The length of the list is not same as the node degree')\n",
    "        break\n",
    "co_authorship_path = path + \"dic_co_authorship_final.npy\"\n",
    "np.save(co_authorship_path, co_authored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1lsxaxYcKjY"
   },
   "source": [
    "#### Co_authorship_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6F0k4GClp0C"
   },
   "source": [
    "This feature describes how many authors collaborate in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tOty21PcICC"
   },
   "outputs": [],
   "source": [
    "# Gather all documents in same list from all authors\n",
    "docs_id =[id for docs in d.values() for id in docs ] \n",
    "# Dictionary of document occurences\n",
    "docs_count = {}\n",
    "for id in docs_id:\n",
    "    if id in docs_count:\n",
    "        docs_count[id] +=1\n",
    "    else:\n",
    "        doc_count_id[id] = 1\n",
    "# Dictionary of authors and their document occurences\n",
    "auth_abstract_count = {}\n",
    "for auth in d:\n",
    "    count = []\n",
    "    for doc in d[auth]:\n",
    "        if doc in doc_count_id:\n",
    "            count.append(doc_count_id[doc])\n",
    "        else:\n",
    "            count.append(0)\n",
    "    auth_abstract_count[auth] = count\n",
    "# Save it\n",
    "np.save(path+'auth_num_docs.npy', auth_abstract_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pRI0jDQl7BK"
   },
   "source": [
    "## Corpus_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvDom8oG861j"
   },
   "source": [
    "It describes the ratio of the unique words in an abstract to the corpus words for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGnLLLNbl9BT",
    "outputId": "04130e3f-effa-4e59-ff03-08f58a5dc28b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1056539/1056539 [00:30<00:00, 34658.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Gather unique tokens in the corpus\n",
    "corpus=[]\n",
    "for val in tqdm(d_abstracts.values()):\n",
    "    for token in val.split(','):\n",
    "        if len(token)>0:\n",
    "            corpus.append(token)\n",
    "corpus_unique = set(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBNUd0ZfmXKB",
    "outputId": "fdd652d0-9c8d-4b18-875e-3902796b409f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231239/231239 [00:04<00:00, 47524.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# The ratio of unique token in each document abstract to the corpus\n",
    "doc_abstract_count = {}\n",
    "for id in d_abstracts:\n",
    "    abstract_ = set(d_abstracts[id].split(','))\n",
    "    doc_abstract_count[id] = len(abstract_)/len(corpus_unique)\n",
    "\n",
    "papers_ratio_corpus = {}\n",
    "for id in tqdm(d):\n",
    "    count = []\n",
    "    for doc in d[id]:\n",
    "        if doc not in doc_abstract_count:\n",
    "            count.append(0.)\n",
    "        else:\n",
    "            count.append(doc_abstract_count[doc])\n",
    "    papers_ratio_corpus[id] = count\n",
    "#Save it\n",
    "np.save(path+'Atr_words_count.npy', papers_ratio_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CB8G4m6opse"
   },
   "source": [
    "##\tAbstract_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7yl4h8F8-LV"
   },
   "source": [
    "It is the percentage of unique words in each author’s abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enD_w6iSoesq"
   },
   "outputs": [],
   "source": [
    "abstract_words = {}\n",
    "for id in d_abstracts:\n",
    "    abstract_ = d_abstracts[id].split(',')\n",
    "    abstract_words[id] = len(set(abstract_))/len(abstract_)\n",
    "\n",
    "papers_ratio = {}\n",
    "for id in tqdm(d):\n",
    "    count = []\n",
    "    for doc in d[id]:\n",
    "        if doc not in abstract_words:\n",
    "            count.append(0.)\n",
    "        else:\n",
    "            count.append(abstract_words[doc])\n",
    "    papers_ratio[id] = count\n",
    "#Save it\n",
    "np.save(path+'Atr_coprs_ratio.npy', papers_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEF-Nkhw_aOT"
   },
   "source": [
    "## Abstract and Corpus frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y16YK9uS_0Tr"
   },
   "source": [
    "It is  frequency of tokens that are repeated at least twice in the abstract and then in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9nDlERX_fCh"
   },
   "outputs": [],
   "source": [
    "doc_freq_abs = {}\n",
    "doc_freq_crps = {}\n",
    "for doc in tqdm(doc_freq):\n",
    "    if len(doc_freq[doc]) ==0:\n",
    "        doc_freq_abs[doc] = 0\n",
    "        doc_freq_crps[doc] = 0\n",
    "    else: \n",
    "        doc_freq_abs[doc] = len(doc_freq[doc])/len(set(d_abstracts[doc]))\n",
    "        doc_freq_crps[doc] = len(doc_freq[doc])/len(corpus)\n",
    "\n",
    "auth_freq_abs = {}\n",
    "auth_freq_crps = {}\n",
    "for auth in d:\n",
    "    count_abs = []\n",
    "    count_crps = []\n",
    "    for doc in d[auth]:\n",
    "        if doc in doc_freq_abs:\n",
    "            count_abs.append(doc_freq_abs[doc])\n",
    "            count_crps.append(doc_freq_crps[doc])\n",
    "    auth_freq_abs[auth] = count_abs\n",
    "    auth_freq_crps[auth] =  count_crps\n",
    "\n",
    "np.save(path+'auth_freq_abs.npy',auth_freq_abs)\n",
    "np.save(path+'auth_freq_crps.npy',auth_freq_crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qlsxWbcv9ei"
   },
   "source": [
    "## Doc_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwAPIuiq9P3X"
   },
   "source": [
    "It is the language of each document for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-sPsIv0q0N2"
   },
   "outputs": [],
   "source": [
    "doc_lang = {}\n",
    "for doc in doc_abst:\n",
    "    doc_lang[doc] = detect(doc_abst[doc])\n",
    "\n",
    "author_lang = {}\n",
    "for id in tqdm(d):\n",
    "    lang = []\n",
    "    for doc in d[id]:\n",
    "        if doc not in doc_lang:\n",
    "            continue\n",
    "        else:\n",
    "            lang.append(doc_lang[doc])\n",
    "    author_lang[id] = lang\n",
    "\n",
    "#Save it\n",
    "np.save(path+'authors_langage.npy',author_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgHisSPCXfZB"
   },
   "source": [
    "# Data Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WT0qnWFX5Hj"
   },
   "source": [
    "The features explained above were generated and exported to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4S5p4XiEW0hQ"
   },
   "outputs": [],
   "source": [
    "authors_count = np.load(path+'auth_num_docs.npy',allow_pickle='TRUE').item()\n",
    "papers_ratio_corpus = np.load(path+'Atr_words_count.npy',allow_pickle='TRUE').item()\n",
    "papers_ratio = np.load(path+'Atr_coprs_ratio.npy',allow_pickle='TRUE').item()\n",
    "authors_lang = np.load(path+'authors_langage.npy',allow_pickle='TRUE').item()\n",
    "co_author = np.load(path+'dic_co_authorship_final.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWXOL4VUZivw"
   },
   "outputs": [],
   "source": [
    "n_feature = 20\n",
    "df_features  = np.zeros((n_nodes,n_feature))\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    df_features[i,0:1] = avg_neighbor_degree[node] #avg_ngbr\n",
    "    df_features[i,1:2] = G.degree(node) #nd_degree\n",
    "    df_features[i,2:3] = core_number[node] #core_n\n",
    "    df_features[i,3:4] = cluster[node] #cluster\n",
    "    df_features[i,4:5] = len(d[str(int(node))]) #docs_n\n",
    "    df_features[i,5:6] = authors_count[str(int(node))].count(1) #one_co_authorship_2\n",
    "    df_features[i,6:7] = np.max(papers_ratio_corpus[str(int(node))]) #max_papers_ratio_corpus\n",
    "    df_features[i,7:8] = np.mean(papers_ratio_corpus[str(int(node))]) #mean_papers_ratio_corpus\n",
    "    df_features[i,8:9] = np.min(papers_ratio_corpus[str(int(node))]) # min_papers_ratio_corpus\n",
    "    df_features[i,9:10] = np.max(papers_ratio[str(int(node))]) # max_papers_ratio\n",
    "    df_features[i,10:11] = np.mean(papers_ratio[str(int(node))]) #mean_papers_ratio\n",
    "    df_features[i,11:12] = np.min(papers_ratio[str(int(node))]) #min_papers_ratio\n",
    "    df_features[i,12:13] = np.sum([x for x in authors_count[str(int(node))] if x!=1]) #sum_co_authorship_2\n",
    "    df_features[i,13:14] = np.max(authors_count[str(int(node))]) #max_co_authorship_2\n",
    "    df_features[i,14:15] = int(np.round(np.mean(authors_count[str(int(node))]))) #mean_co_authorship_2\n",
    "    df_features[i,15:16] = np.min(authors_count[str(int(node))]) # min_co_authorship_2\n",
    "    df_features[i,16:17] = np.mean(co_author[int(node)]) #mean_co_authorship_1\n",
    "    df_features[i,17:18] = np.max(co_author[int(node)]) #max_co_authorship_1\n",
    "    df_features[i,18:19] = co_author[int(node)].count(2) #two_co_authorship_1\n",
    "    df_features[i,19:20] = len(set(authors_lang[str(int(node))])) #abstract languages\n",
    "    df_features[i,20:21] = int(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9Stj_e1AD46"
   },
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(df_features)\n",
    "df_features.columns = ['avg_ngbr', 'nd_degr', 'core_n', 'cluster', 'docs_n',\n",
    "       'one_co_authorship_2', 'max_papers_ratio_corpus',\n",
    "       'mean_papers_ratio_corpus', 'min_papers_ratio_corpus',\n",
    "       'max_papers_ratio', 'mean_papers_ratio', 'min_papers_ratio',\n",
    "       'sum_co_authorship_2', 'max_co_authorship_2', 'mean_co_authorship_2',\n",
    "       'min_co_authorship_2', 'mean_co_authorship_1', 'max_co_authorship_1',\n",
    "       'two_co_authorship_1','auth_lang_n','author_id']\n",
    "df_features.head(5)\n",
    "df_features.to_csv(path+'df_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Features engineering",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
